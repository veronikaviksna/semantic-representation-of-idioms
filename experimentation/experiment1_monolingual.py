# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WHA237-EBfm1Z7qigf_nd2qiNAMtESVt
"""

# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score

from experimentation.metrics import (
    cosine_sim,
    mann_whitney_test,
    cohens_d,
    pca_analysis,
)

# ============================================================
# Competitive Precision@k (local, safe)
# ============================================================

def precision_at_k_competitive(
    query_embeddings: np.ndarray,
    query_labels: np.ndarray,
    candidate_embeddings: np.ndarray,
    candidate_labels: np.ndarray,
    k: int,
) -> float:
    """
    Competitive Precision@k with exclusion of self-matches.
    """
    sims = query_embeddings @ candidate_embeddings.T

    # Exclude self-matches if queries are subset of candidates
    if query_embeddings.shape[0] <= candidate_embeddings.shape[0]:
        for i, q in enumerate(query_embeddings):
            # find identical vector (safe for subset sampling)
            matches = np.all(candidate_embeddings == q, axis=1)
            sims[i, matches] = -np.inf

    topk = np.argsort(-sims, axis=1)[:, :k]
    hits = (candidate_labels[topk] == query_labels[:, None]).any(axis=1)
    return hits.mean()


# ============================================================
# Experiment 1: Monolingual (MAGPIE)
# ============================================================

def experiment1_monolingual(
    embeddings: np.ndarray,
    df_magpie: pd.DataFrame,
    max_pairs: int = 5000,
    pca_sample_size: int = 3000,
    precision_subset_size: int = 1000,
    seed: int = 42,
):
    """
    Memory-safe monolingual experiment for MAGPIE.
    Tests whether embeddings distinguish literal vs figurative usage.
    """

    rng = np.random.default_rng(seed)
    labels = df_magpie["usage"].values

    # --------------------------------------------------
    # Split embeddings by usage
    # --------------------------------------------------
    emb_fig = embeddings[labels == "figurative"]
    emb_lit = embeddings[labels == "literal"]

    # --------------------------------------------------
    # Similarity sampling (memory-safe)
    # --------------------------------------------------
    def sample_pairs(A, B, n):
        idx_a = rng.integers(0, len(A), size=n)
        idx_b = rng.integers(0, len(B), size=n)
        return cosine_sim(A[idx_a], B[idx_b])

    n = min(max_pairs, len(emb_fig), len(emb_lit))

    sim_fig = sample_pairs(emb_fig, emb_fig, n)
    sim_lit = sample_pairs(emb_lit, emb_lit, n)
    sim_cross = sample_pairs(emb_fig, emb_lit, n)

    # --------------------------------------------------
    # Statistical tests
    # --------------------------------------------------
    statistics = {
        "fig_vs_cross": mann_whitney_test(
            sim_fig, sim_cross, alternative="greater"
        ),
        "lit_vs_cross": mann_whitney_test(
            sim_lit, sim_cross, alternative="greater"
        ),
    }

    effect_sizes = {
        "d_fig": cohens_d(sim_fig, sim_cross),
        "d_lit": cohens_d(sim_lit, sim_cross),
    }

    # --------------------------------------------------
    # Precision@k (global, competitive)
    # --------------------------------------------------
    global_size = min(precision_subset_size, len(embeddings))
    idx_global = rng.choice(len(embeddings), size=global_size, replace=False)

    precision_at_k_global = {
        k: precision_at_k_competitive(
            embeddings[idx_global],
            labels[idx_global],
            embeddings,
            labels,
            k,
        )
        for k in [1, 5, 10]
    }

    # --------------------------------------------------
    # Precision@k by usage (competitive)
    # --------------------------------------------------
    def precision_by_usage(emb_query, label_name):
        size = min(precision_subset_size, len(emb_query))
        idx = rng.choice(len(emb_query), size=size, replace=False)
        query_labels = np.array([label_name] * size)

        return {
            k: precision_at_k_competitive(
                emb_query[idx],
                query_labels,
                embeddings,
                labels,
                k,
            )
            for k in [1, 5, 10]
        }

    precision_by_usage_results = {
        "figurative": precision_by_usage(emb_fig, "figurative"),
        "literal": precision_by_usage(emb_lit, "literal"),
    }

    # --------------------------------------------------
    # PCA + silhouette score
    # --------------------------------------------------
    pca_size = min(pca_sample_size, len(embeddings))
    idx_pca = rng.choice(len(embeddings), size=pca_size, replace=False)

    pca = pca_analysis(embeddings[idx_pca])

    labels_binary = (labels[idx_pca] == "figurative").astype(int)

    sil_score = silhouette_score(
        embeddings[idx_pca],
        labels_binary,
        metric="cosine",
    )

    # --------------------------------------------------
    # Results
    # --------------------------------------------------
    results = {
        "similarities": {
            "fig": sim_fig,
            "lit": sim_lit,
            "cross": sim_cross,
        },
        "means": {
            "fig_fig": float(sim_fig.mean()),
            "lit_lit": float(sim_lit.mean()),
            "fig_lit": float(sim_cross.mean()),
        },
        "statistics": statistics,
        "effect_sizes": effect_sizes,
        "precision_at_k": precision_at_k_global,
        "precision_at_k_by_usage": precision_by_usage_results,
        "pca": {
            "explained_variance": pca["explained_variance"],
            "total_variance": pca["total_variance"],
            "silhouette_score": float(sil_score),
        },
    }

    return results