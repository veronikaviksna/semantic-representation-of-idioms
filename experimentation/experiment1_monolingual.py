# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U2pBZtGUZxNYr3PXcuDFhROduVtr_-Qb
"""

# -*- coding: utf-8 -*-
"""
Experiment 1: Monolingual Analysis (MAGPIE)
Fixed version with correct similarity sampling
"""

import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity

from experimentation.metrics import (
    mann_whitney_test,
    cohens_d,
    pca_analysis,
)


# ============================================================
# Competitive Precision@k (safe with better self-match detection)
# ============================================================

def precision_at_k_competitive(
    query_embeddings: np.ndarray,
    query_labels: np.ndarray,
    candidate_embeddings: np.ndarray,
    candidate_labels: np.ndarray,
    k: int,
) -> float:
    """
    Competitive Precision@k with proper exclusion of self-matches.

    Args:
        query_embeddings: embeddings to use as queries (N_q, D)
        query_labels: labels for queries (N_q,)
        candidate_embeddings: all embeddings to search in (N_c, D)
        candidate_labels: labels for all candidates (N_c,)
        k: number of neighbors to retrieve

    Returns:
        mean precision@k across all queries
    """
    sims = query_embeddings @ candidate_embeddings.T

    # Exclude self-matches using cosine distance threshold
    # Two embeddings are "the same" if their distance is < 1e-6
    for i, q in enumerate(query_embeddings):
        distances = np.linalg.norm(candidate_embeddings - q, axis=1)
        self_matches = distances < 1e-6
        sims[i, self_matches] = -np.inf

    # Get top-k indices
    topk = np.argsort(-sims, axis=1)[:, :k]

    # Check if any of top-k has same label
    hits = (candidate_labels[topk] == query_labels[:, None]).any(axis=1)

    return float(hits.mean())


# ============================================================
# Experiment 1: Monolingual (MAGPIE) - FIXED VERSION
# ============================================================

def experiment1_monolingual(
    embeddings: np.ndarray,
    df_magpie: pd.DataFrame,
    similarity_sample_size: int = 1000,
    pca_sample_size: int = 3000,
    precision_subset_size: int = 1000,
    seed: int = 42,
):
    """
    Memory-safe monolingual experiment for MAGPIE.
    Tests whether embeddings distinguish literal vs figurative usage.

    FIXED: Properly excludes self-matches when computing within-usage similarities.

    Args:
        embeddings: sentence embeddings (N, D)
        df_magpie: MAGPIE dataframe with 'usage' column
        similarity_sample_size: number of embeddings to use for similarity analysis
        pca_sample_size: number of embeddings to use for PCA
        precision_subset_size: number of queries for precision@k evaluation
        seed: random seed

    Returns:
        dict with all results
    """
    rng = np.random.default_rng(seed)
    labels = df_magpie["usage"].values

    print(f"\n[Experiment 1] Starting monolingual analysis...")
    print(f"  Total samples: {len(embeddings)}")
    print(f"  Figurative: {(labels == 'figurative').sum()}")
    print(f"  Literal: {(labels == 'literal').sum()}")

    # --------------------------------------------------
    # Split embeddings by usage
    # --------------------------------------------------
    emb_fig = embeddings[labels == "figurative"]
    emb_lit = embeddings[labels == "literal"]

    # --------------------------------------------------
    # Similarity analysis (FIXED - no self-matches!)
    # --------------------------------------------------
    print(f"\n[Step 1] Computing similarity distributions...")

    N_sim = min(similarity_sample_size, len(emb_fig), len(emb_lit))
    print(f"  Using N={N_sim} samples for similarity computation")

    # Compute full similarity matrices for subsets
    sim_fig_fig = cosine_similarity(emb_fig[:N_sim], emb_fig[:N_sim]).flatten()
    sim_lit_lit = cosine_similarity(emb_lit[:N_sim], emb_lit[:N_sim]).flatten()
    sim_fig_lit = cosine_similarity(emb_fig[:N_sim], emb_lit[:N_sim]).flatten()

    # Remove self-similarities (diagonal elements have similarity ≈ 1.0)
    sim_fig = sim_fig_fig[sim_fig_fig < 0.999]
    sim_lit = sim_lit_lit[sim_lit_lit < 0.999]
    sim_cross = sim_fig_lit  # No self-matches possible here

    print(f"  Figurative pairs (after removing self-matches): {len(sim_fig)}")
    print(f"  Literal pairs (after removing self-matches): {len(sim_lit)}")
    print(f"  Cross-usage pairs: {len(sim_cross)}")

    # Debug: show means
    print(f"\n  Similarity means:")
    print(f"    Fig-Fig: {sim_fig.mean():.4f} ± {sim_fig.std():.4f}")
    print(f"    Lit-Lit: {sim_lit.mean():.4f} ± {sim_lit.std():.4f}")
    print(f"    Fig-Lit: {sim_cross.mean():.4f} ± {sim_cross.std():.4f}")

    # --------------------------------------------------
    # Statistical tests
    # --------------------------------------------------
    print(f"\n[Step 2] Statistical significance tests...")

    statistics = {
        "fig_vs_cross": mann_whitney_test(
            sim_fig, sim_cross, alternative="greater"
        ),
        "lit_vs_cross": mann_whitney_test(
            sim_lit, sim_cross, alternative="greater"
        ),
    }

    effect_sizes = {
        "d_fig": cohens_d(sim_fig, sim_cross),
        "d_lit": cohens_d(sim_lit, sim_cross),
    }

    print(f"  Fig vs Cross: U={statistics['fig_vs_cross'][0]:.1f}, "
          f"p={statistics['fig_vs_cross'][1]:.2e}, d={effect_sizes['d_fig']:.3f}")
    print(f"  Lit vs Cross: U={statistics['lit_vs_cross'][0]:.1f}, "
          f"p={statistics['lit_vs_cross'][1]:.2e}, d={effect_sizes['d_lit']:.3f}")

    # Sanity checks
    try:
        assert sim_fig.mean() > sim_cross.mean(), \
            "Expected fig-fig > cross, but got reversed!"
        assert sim_lit.mean() > sim_cross.mean(), \
            "Expected lit-lit > cross, but got reversed!"
        assert effect_sizes["d_fig"] > 0, \
            "Expected positive effect size for figurative!"
        print(f"  ✓ Sanity checks passed")
    except AssertionError as e:
        print(f"  ⚠ Warning: {e}")

    # --------------------------------------------------
    # Precision@k (global, competitive)
    # --------------------------------------------------
    print(f"\n[Step 3] Computing global Precision@k...")

    global_size = min(precision_subset_size, len(embeddings))
    idx_global = rng.choice(len(embeddings), size=global_size, replace=False)

    precision_at_k_global = {}
    for k in [1, 5, 10]:
        p = precision_at_k_competitive(
            embeddings[idx_global],
            labels[idx_global],
            embeddings,
            labels,
            k,
        )
        precision_at_k_global[k] = p
        print(f"  Precision@{k}: {p:.3f}")

    # --------------------------------------------------
    # Precision@k by usage (competitive)
    # --------------------------------------------------
    print(f"\n[Step 4] Computing Precision@k by usage...")

    def precision_by_usage(emb_query, label_name):
        size = min(precision_subset_size, len(emb_query))
        idx = rng.choice(len(emb_query), size=size, replace=False)
        query_labels = np.array([label_name] * size)

        results = {}
        for k in [1, 5, 10]:
            p = precision_at_k_competitive(
                emb_query[idx],
                query_labels,
                embeddings,
                labels,
                k,
            )
            results[k] = p
        return results

    precision_by_usage_results = {
        "figurative": precision_by_usage(emb_fig, "figurative"),
        "literal": precision_by_usage(emb_lit, "literal"),
    }

    print(f"  Figurative:")
    for k, v in precision_by_usage_results["figurative"].items():
        print(f"    Precision@{k}: {v:.3f}")
    print(f"  Literal:")
    for k, v in precision_by_usage_results["literal"].items():
        print(f"    Precision@{k}: {v:.3f}")

    # --------------------------------------------------
    # PCA + silhouette score
    # --------------------------------------------------
    print(f"\n[Step 5] PCA analysis...")

    pca_size = min(pca_sample_size, len(embeddings))
    idx_pca = rng.choice(len(embeddings), size=pca_size, replace=False)

    pca = pca_analysis(embeddings[idx_pca])

    labels_binary = (labels[idx_pca] == "figurative").astype(int)

    sil_score = silhouette_score(
        embeddings[idx_pca],
        labels_binary,
        metric="cosine",
    )

    print(f"  Explained variance (PC1, PC2): {pca['explained_variance']}")
    print(f"  Total variance: {pca['total_variance']:.4f}")
    print(f"  Silhouette score: {sil_score:.4f}")

    # --------------------------------------------------
    # Results
    # --------------------------------------------------
    results = {
        "similarities": {
            "fig": sim_fig,
            "lit": sim_lit,
            "cross": sim_cross,
        },
        "means": {
            "fig_fig": float(sim_fig.mean()),
            "lit_lit": float(sim_lit.mean()),
            "fig_lit": float(sim_cross.mean()),
        },
        "statistics": statistics,
        "effect_sizes": effect_sizes,
        "precision_at_k": precision_at_k_global,
        "precision_at_k_by_usage": precision_by_usage_results,
        "pca": {
            "explained_variance": pca["explained_variance"],
            "total_variance": pca["total_variance"],
            "silhouette_score": float(sil_score),
        },
    }

    print(f"\n[Experiment 1] Complete!")

    return results