# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q8lsP_ugkYr9VZ2HP8zjaGOgQRa-KLIC
"""

import numpy as np
from scipy.stats import mannwhitneyu
from sklearn.decomposition import PCA


# ============================================================
# Basic similarity utilities
# ============================================================

def cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    Cosine similarity for aligned vectors.
    Assumes embeddings are L2-normalized.
    """
    return np.sum(a * b, axis=1)


def random_pair_similarity(
    emb_a: np.ndarray,
    emb_b: np.ndarray,
    seed: int = 42
) -> np.ndarray:
    """
    Cosine similarity for randomly paired vectors.
    """
    rng = np.random.default_rng(seed)
    idx = rng.permutation(len(emb_b))
    return cosine_sim(emb_a, emb_b[idx])


# ============================================================
# Statistical tests
# ============================================================

def mann_whitney_test(x, y, alternative="two-sided"):
    """
    Mannâ€“Whitney U test.
    """
    return mannwhitneyu(x, y, alternative=alternative)


def cohens_d(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Cohen's d effect size.
    """
    nx, ny = len(x), len(y)
    pooled_std = np.sqrt(
        ((nx - 1) * np.var(x, ddof=1) +
         (ny - 1) * np.var(y, ddof=1)) / (nx + ny - 2)
    )
    return (np.mean(x) - np.mean(y)) / pooled_std


# ============================================================
# Retrieval / neighborhood metrics
# ============================================================

def precision_at_k(
    query_embeddings: np.ndarray,
    candidate_embeddings: np.ndarray,
    labels: np.ndarray,
    k: int
) -> float:
    """
    Precision@k for same-label neighborhood retrieval,
    excluding self-matches.
    """
    sims = query_embeddings @ candidate_embeddings.T

    # Exclude self-similarity when queries == candidates
    if query_embeddings.shape == candidate_embeddings.shape:
        np.fill_diagonal(sims, -np.inf)

    topk = np.argsort(-sims, axis=1)[:, :k]
    hits = (labels[topk] == labels[:, None]).any(axis=1)

    return hits.mean()


def retrieval_accuracy(
    emb_src: np.ndarray,
    emb_tgt: np.ndarray,
    k: int = 1
) -> float:
    """
    Cross-lingual retrieval accuracy.
    Assumes aligned pairs (i -> i).
    """
    sims = emb_src @ emb_tgt.T
    topk = np.argsort(-sims, axis=1)[:, :k]
    correct = np.arange(len(emb_src))
    return np.mean([c in topk[i] for i, c in enumerate(correct)])


# ============================================================
# Dimensionality reduction
# ============================================================

def pca_analysis(
    X: np.ndarray,
    n_components: int = 2
) -> dict:
    """
    Perform PCA and return projections and explained variance.
    """
    pca = PCA(n_components=n_components)
    X_proj = pca.fit_transform(X)

    return {
        "X_proj": X_proj,
        "explained_variance": pca.explained_variance_ratio_,
        "total_variance": pca.explained_variance_ratio_.sum(),
    }