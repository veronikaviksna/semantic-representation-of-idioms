# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WHA237-EBfm1Z7qigf_nd2qiNAMtESVt
"""

# -*- coding: utf-8 -*-
"""
Experiment 2: Cross-lingual Alignment (IdiomsInCtx-MT)
Fixed version with better logging and validation
"""

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score

from experimentation.metrics import (
    cosine_sim,
    random_pair_similarity,
    cohens_d,
    mann_whitney_test,
    retrieval_accuracy,
    pca_analysis,
)


def experiment2_crosslingual(
    emb_ru: np.ndarray,
    emb_en: np.ndarray,
    sentences_ru: list[str],
    sentences_en: list[str],
    seed: int = 42,
):
    """
    Cross-lingual alignment experiment (RU–EN).

    Tests whether idiomatic meaning is preserved across Russian and English.
    Compares LaBSE embeddings against a TF-IDF lexical baseline.

    Args:
        emb_ru: Russian embeddings (N, D)
        emb_en: English embeddings (N, D)
        sentences_ru: Russian sentences (for TF-IDF)
        sentences_en: English sentences (for TF-IDF)
        seed: random seed

    Returns:
        dict with all results
    """
    rng = np.random.default_rng(seed)

    N = len(emb_ru)

    print(f"\n[Experiment 2] Starting cross-lingual analysis...")
    print(f"  Total sentence pairs: {N}")

    # Validation
    assert len(emb_ru) == len(emb_en), "RU and EN embeddings must be aligned!"
    assert len(sentences_ru) == len(sentences_en), "Sentence lists must be aligned!"
    assert emb_ru.shape[1] == emb_en.shape[1], "Embedding dimensions must match!"

    # --------------------------------------------------
    # LaBSE similarity distributions
    # --------------------------------------------------
    print(f"\n[Step 1] Computing LaBSE similarity distributions...")

    aligned = cosine_sim(emb_ru, emb_en)
    random = random_pair_similarity(emb_ru, emb_en, seed=seed)

    print(f"  Aligned similarity: {aligned.mean():.4f} ± {aligned.std():.4f}")
    print(f"  Random similarity: {random.mean():.4f} ± {random.std():.4f}")
    print(f"  Difference: {aligned.mean() - random.mean():.4f}")

    # --------------------------------------------------
    # Statistical tests
    # --------------------------------------------------
    print(f"\n[Step 2] Statistical significance tests...")

    u_stat, p_value = mann_whitney_test(aligned, random, alternative="greater")
    d = cohens_d(aligned, random)

    print(f"  Mann-Whitney U: {u_stat:.1f}, p={p_value:.2e}")
    print(f"  Cohen's d: {d:.3f}")

    # Sanity checks
    try:
        assert aligned.mean() > random.mean(), \
            "Expected aligned > random, but got reversed!"
        assert d > 5.0, \
            f"Expected large effect size (>5), got {d:.3f}"
        assert p_value < 0.001, \
            f"Expected significant result, got p={p_value:.3e}"
        print(f"  ✓ Sanity checks passed")
    except AssertionError as e:
        print(f"  ⚠ Warning: {e}")

    # --------------------------------------------------
    # Translation retrieval
    # --------------------------------------------------
    print(f"\n[Step 3] Translation retrieval accuracy...")

    retrieval = {}
    for k in [1, 5, 10]:
        acc = retrieval_accuracy(emb_ru, emb_en, k)
        retrieval[k] = acc
        print(f"  Precision@{k}: {acc:.3f}")

    # Sanity check
    try:
        assert retrieval[1] > 0.95, \
            f"Expected high Top-1 accuracy, got {retrieval[1]:.3f}"
        print(f"  ✓ Retrieval sanity check passed")
    except AssertionError as e:
        print(f"  ⚠ Warning: {e}")

    # --------------------------------------------------
    # TF-IDF baseline (non-semantic)
    # --------------------------------------------------
    print(f"\n[Step 4] Computing TF-IDF baseline...")

    all_sentences = sentences_ru + sentences_en

    vectorizer = TfidfVectorizer(
        max_features=20000,
        ngram_range=(1, 2),
        lowercase=True,
    )

    X = vectorizer.fit_transform(all_sentences)

    X_ru = X[:len(sentences_ru)]
    X_en = X[len(sentences_ru):]

    print(f"  TF-IDF vocabulary size: {len(vectorizer.vocabulary_)}")

    # Aligned pairs
    tfidf_aligned = cosine_similarity(X_ru, X_en).diagonal()

    # Random pairs
    perm = rng.permutation(X_en.shape[0])
    tfidf_random = cosine_similarity(X_ru, X_en[perm]).diagonal()

    print(f"  TF-IDF aligned: {tfidf_aligned.mean():.4f} ± {tfidf_aligned.std():.4f}")
    print(f"  TF-IDF random: {tfidf_random.mean():.4f} ± {tfidf_random.std():.4f}")

    # Comparison with LaBSE
    print(f"\n  Comparison (LaBSE vs TF-IDF):")
    print(f"    LaBSE aligned: {aligned.mean():.4f}")
    print(f"    TF-IDF aligned: {tfidf_aligned.mean():.4f}")
    print(f"    Ratio: {aligned.mean() / max(tfidf_aligned.mean(), 1e-6):.1f}x")

    # Sanity check
    try:
        assert aligned.mean() > tfidf_aligned.mean() * 10, \
            "Expected LaBSE >> TF-IDF, but difference is small"
        print(f"  ✓ LaBSE substantially outperforms TF-IDF")
    except AssertionError as e:
        print(f"  ⚠ Warning: {e}")

    # --------------------------------------------------
    # PCA (LaBSE only)
    # --------------------------------------------------
    print(f"\n[Step 5] Cross-lingual PCA analysis...")

    X_labse = np.vstack([emb_ru, emb_en])
    pca = pca_analysis(X_labse)

    print(f"  Explained variance (PC1, PC2): {pca['explained_variance']}")
    print(f"  Total variance: {pca['total_variance']:.4f}")

    # Compute silhouette score (how separated are RU vs EN?)
    lang_labels = np.array([0] * N + [1] * N)  # 0=RU, 1=EN

    # Use PCA space for silhouette (faster)
    from sklearn.decomposition import PCA
    pca_model = PCA(n_components=2)
    X_pca = pca_model.fit_transform(X_labse)
    sil_score = silhouette_score(X_pca, lang_labels, metric="euclidean")

    print(f"  Silhouette score (PCA space): {sil_score:.4f}")
    print(f"    (Low score indicates language-agnostic representations)")

    # --------------------------------------------------
    # Results
    # --------------------------------------------------
    results = {
        # Similarity distributions
        "aligned_similarities": aligned,
        "random_similarities": random,

        # Summary statistics
        "aligned_mean": float(aligned.mean()),
        "aligned_std": float(aligned.std()),
        "random_mean": float(random.mean()),
        "random_std": float(random.std()),

        # Statistical tests
        "mann_whitney_u": u_stat,
        "mann_whitney_p": p_value,
        "cohens_d": d,

        # Retrieval
        "retrieval": retrieval,

        # TF-IDF baseline
        "tfidf_aligned_mean": float(tfidf_aligned.mean()),
        "tfidf_aligned_std": float(tfidf_aligned.std()),
        "tfidf_random_mean": float(tfidf_random.mean()),
        "tfidf_random_std": float(tfidf_random.std()),

        # PCA
        "pca": {
            "explained_variance": pca["explained_variance"],
            "total_variance": pca["total_variance"],
            "silhouette_score": float(sil_score),
            "X_proj": pca["X_proj"],  # for plotting
        },
    }

    print(f"\n[Experiment 2] Complete!")

    return results